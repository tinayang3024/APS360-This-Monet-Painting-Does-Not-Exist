# -*- coding: utf-8 -*-
"""CycleGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rzaEBv6J7s--a4NCtxFjItAQJOMDGHtE

# Import Libraries
"""

import torchvision
import torch
import itertools
import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler
import time
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
import random
import torch

"""# CycleGAN Utils"""

class ImagePool():
    """This class implements an image buffer that stores previously generated images.
    This buffer enables us to update discriminators using a history of generated images
    rather than the ones produced by the latest generators.
    """

    def __init__(self, pool_size):
        """Initialize the ImagePool class
        Parameters:
            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created
        """
        self.pool_size = pool_size
        if self.pool_size > 0:  # create an empty pool
            self.num_imgs = 0
            self.images = []

    def query(self, images):
        """Return an image from the pool.
        Parameters:
            images: the latest generated images from the generator
        Returns images from the buffer.
        By 50/100, the buffer will return input images.
        By 50/100, the buffer will return images previously stored in the buffer,
        and insert the current images to the buffer.
        """
        if self.pool_size == 0:  # if the buffer size is 0, do nothing
            return images
        return_images = []
        for image in images:
            image = torch.unsqueeze(image.data, 0)
            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer
                self.num_imgs = self.num_imgs + 1
                self.images.append(image)
                return_images.append(image)
            else:
                p = random.uniform(0, 1)
                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer
                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive
                    tmp = self.images[random_id].clone()
                    self.images[random_id] = image
                    return_images.append(tmp)
                else:       # by another 50% chance, the buffer will return the current image
                    return_images.append(image)
        return_images = torch.cat(return_images, 0)   # collect all the images and return
        return return_images

class GANLoss(nn.Module):
    """Define different GAN objectives.
    The GANLoss class abstracts away the need to create the target label tensor
    that has the same size as the input.
    """

    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):
        """ Initialize the GANLoss class.
        Parameters:
            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.
            target_real_label (bool) - - label for a real image
            target_fake_label (bool) - - label of a fake image
        Note: Do not use sigmoid as the last layer of Discriminator.
        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.
        """
        super(GANLoss, self).__init__()
        self.register_buffer('real_label', torch.tensor(target_real_label))
        self.register_buffer('fake_label', torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        # print("fake x pool set to : " + str(self.fake_x_pool))
        if gan_mode == 'lsgan':
            self.loss = nn.MSELoss()
        elif gan_mode == 'vanilla':
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode in ['wgangp']:
            self.loss = None
        else:
            raise NotImplementedError('gan mode %s not implemented' % gan_mode)

    def get_target_tensor(self, prediction, target_is_real):
        """Create label tensors with the same size as the input.
        Parameters:
            prediction (tensor) - - tpyically the prediction from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images
        Returns:
            A label tensor filled with ground truth label, and with the size of the input
        """
        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)

    def __call__(self, prediction, target_is_real):
        """Calculate loss given Discriminator's output and grount truth labels.
        Parameters:
            prediction (tensor) - - tpyically the prediction output from a discriminator
            target_is_real (bool) - - if the ground truth label is for real images or fake images
        Returns:
            the calculated loss.
        """
        if self.gan_mode in ['lsgan', 'vanilla']:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            loss = self.loss(prediction, target_tensor)
        elif self.gan_mode == 'wgangp':
            if target_is_real:
                loss = -prediction.mean()
            else:
                loss = prediction.mean()
        return loss

"""# CycleGAN Architecture"""

class CycleGAN(nn.Module):
  # opt contains: 
  #   lambda_identity: used for scaling the weight of the identity mapping loss
  #   lambda_X: weight for cycle loss (X -> Y -> X)
  #   lambda_Y: weight for cycle loss (Y -> X -> Y)
  def __init__(self, name, opt):
    super(CycleGAN, self).__init__()
    self.name = "CycleGAN" + name

    self.opt = opt
    self.gpu_ids = opt.gpu_ids
    self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')

    #   cycle_x: loss of generating fake x
    #   cycle_y: loss of generating fake x
    #   gan_f: loss of discriminating real x
    #   gan_g: loss of discriminating real y
    self.loss_names = ['cycle_x', 'cycle_y', 'gan_f', 'gan_g']

    self.fake_x_pool = ImagePool(opt.pool_size)
    self.fake_y_pool = ImagePool(opt.pool_size)
    
    #   D_x: discriminator to discriminate real x from fake x
    #   D_y: discriminator to discriminate real y from fake y
    #   G_x: generator to generate fake x
    #   G_y: generator to generate fake y
    self.model_names = ['D_x', 'D_y', 'G_x', 'G_y']
    self.netD_x = MonaiDiscriminator(3)
    self.netD_y = MonaiDiscriminator(3)
    self.netG_x = MonaiGenerator(3, 3, 64, 6, 'instance', True)
    self.netG_y = MonaiGenerator(3, 3, 64, 6, 'instance', True)

    #   criterions
    self.criterionGAN = GANLoss(opt.gan_mode).to(self.device)
    self.criterionCycle = torch.nn.L1Loss()
    
    #   optimizers
    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_x.parameters(), self.netG_y.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_x.parameters(), self.netD_y.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizers = []
    self.optimizers.append(self.optimizer_G)
    self.optimizers.append(self.optimizer_D)

    #   learning rate scheduler
    self.schedulers = [self.get_scheduler(optimizer, opt) for optimizer in self.optimizers]

    #   training records
    self.save_progress = []
    self.all_loss_D_fake = []
    self.all_loss_D_real = []
    self.all_loss_D = []
    self.all_loss_G = []
    self.all_loss_G_x = []
    self.all_loss_G_y = []
    self.all_loss_cycle_x = []
    self.all_loss_cycle_y = []

  def get_scheduler(self, optimizer, opt):
    """Return a learning rate scheduler
    Parameters:
        optimizer          -- the optimizer of the network
        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　
                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine
    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs
    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.
    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.
    See https://pytorch.org/docs/stable/optim.html for more details.
    """
    if opt.lr_policy == 'linear':
        def lambda_rule(epoch):
            lr_l = 1.0 - max(0, epoch - opt.n_epochs)
            return lr_l
        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)
    elif opt.lr_policy == 'step':
        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)
    elif opt.lr_policy == 'plateau':
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)
    elif opt.lr_policy == 'cosine':
        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.n_epochs, eta_min=0)
    else:
        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)
    return scheduler
  
  def update_learning_rate(self):
        """Update learning rates for all the networks; called at the end of every epoch"""
        old_lr = self.optimizers[0].param_groups[0]['lr']
        for scheduler in self.schedulers:
            if self.opt.lr_policy == 'plateau':
                scheduler.step(self.metric)
            else:
                scheduler.step()

        lr = self.optimizers[0].param_groups[0]['lr']
        print('learning rate %.7f -> %.7f' % (old_lr, lr))

  def set_input(self, input):
        """Unpack input data from the dataloader and perform necessary pre-processing steps.
        Parameters:
            input (dict): include the data itself and its metadata information.
        The option 'direction' can be used to swap domain A and domain B.
        """
        XtoY = self.opt.direction == 'XtoY'
        self.real_x = input['X' if XtoY else 'Y']
        self.real_y = input['Y' if XtoY else 'X']

  def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

  def forward(self):
    self.fake_x = self.netG_x(self.real_y)
    self.rec_y = self.netG_y(self.fake_x)   # rec_y = G_y(G_x(y))
    self.fake_y = self.netG_y(self.real_x)
    self.rec_x = self.netG_x(self.fake_y)    # rec_x = G_x(G_y(x))
  
  def backward_D_basic(self, netD, real, fake):
    # real
    pred_real = netD(real)
    self.loss_D_real = self.criterionGAN(pred_real, True)

    # fake
    pred_fake = netD(fake)
    self.loss_D_fake = self.criterionGAN(pred_fake, False)
    

    # combined loss and calculate gradients
    self.loss_D = (self.loss_D_real + self.loss_D_fake) / 2
    self.loss_D.backward()
    return self.loss_D
  
  def backward_D_x(self):
    # Calculate the loss for generators D_x
    fake_x = self.fake_x_pool.query(self.fake_x)
    self.loss_D_x = self.backward_D_basic(self.netD_x, self.real_x, fake_x)

  def backward_D_y(self):
    # Calculate the loss for generators D_y
    fake_y = self.fake_y_pool.query(self.fake_y)
    self.loss_D_y = self.backward_D_basic(self.netD_y, self.real_y, fake_y)

  def backward_G(self):
   # Calculate the loss for generators G_A and G_B
    lambda_x = self.opt.lambda_x
    lambda_y = self.opt.lambda_y

    # GAN loss D_x(G_x(y))
    self.loss_G_x = self.criterionGAN(self.netD_x(self.fake_x), False)
    # GAN loss D_y(G_y(x))
    self.loss_G_y = self.criterionGAN(self.netD_y(self.fake_y), False)
    self.loss_cycle_x = self.criterionCycle(self.rec_x, self.real_x) * lambda_x
    self.loss_cycle_y = self.criterionCycle(self.rec_y, self.real_y) * lambda_y
    
    # combined loss and calculate gradients
    self.loss_G = self.loss_G_x + self.loss_G_y + self.loss_cycle_x + self.loss_cycle_y
    self.loss_G.backward()
  
  def record_loss(self):
    self.all_loss_D_fake.append(self.loss_D_fake)
    self.all_loss_D_real.append(self.loss_D_real)
    self.all_loss_D.append(self.loss_D)
    self.all_loss_G.append(self.loss_G)
    self.all_loss_G_x.append(self.loss_G_x)
    self.all_loss_G_y.append(self.loss_G_y)
    self.all_loss_cycle_x.append(self.loss_cycle_x)
    self.all_loss_cycle_y.append(self.loss_cycle_y)
  
  def record_output_imgs(self):
    # save progress
    progress = {}
    progress["real_x"] = self.real_x
    progress["real_y"] = self.real_y
    progress["fake_x"] = self.fake_x
    progress["rec_y"] = self.rec_y
    progress["fake_y"] = self.fake_y
    progress["rec_x"] = self.rec_x
    self.save_progress.append(progress)

  def optimize_parameters(self):
    """Calculate losses, gradients, and update network weights; called in every training iteration"""
    # forward
    self.forward()      # compute fake images and reconstruction images.
    self.record_output_imgs()  # record generated imgs

    # G_x and G_y
    self.set_requires_grad([self.netD_x, self.netD_y], False)  # Ds require no gradients when optimizing Gs
    self.optimizer_G.zero_grad()  # set G_x and G_y's gradients to zero
    self.backward_G()             # calculate gradients for G_x and G_y
    self.optimizer_G.step()       # update G_x and G_y's weights
    # D_x and D_y
    self.set_requires_grad([self.netD_x, self.netD_y], True)
    self.optimizer_D.zero_grad()   # set D_x and D_y's gradients to zero
    self.backward_D_x()      # calculate gradients for D_x
    self.backward_D_y()      # calculate graidents for D_y

    self.record_loss()      # record losses
    self.optimizer_D.step()  # update D_x and D_y's weights