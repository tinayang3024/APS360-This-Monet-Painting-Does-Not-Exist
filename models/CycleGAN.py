# -*- coding: utf-8 -*-
"""CycleGANModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11hzOKm_EU73Hr03oP65pgkbxiA2yzQ4K
"""

import torchvision
import torch
import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler
import time
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms

#Convolutional Neural Network Architecture
class CycleGANModel(nn.Module):
  # opt contains: 
  #   lambda_identity: used for scaling the weight of the identity mapping loss
  #   lambda_X: weight for cycle loss (X -> Y -> X)
  #   lambda_Y: weight for cycle loss (Y -> X -> Y)
  
  def __init__(self, name, opt):
    super(CycleGAN, self).__init__()
    self.name = "CycleGAN" + name

    #   cycle_x: loss of generating fake x
    #   cycle_y: loss of generating fake x
    #   gan_f: loss of discriminating real x
    #   gan_g: loss of discriminating real y
    self.loss_names = ['cycle_x', 'cycle_y', 'gan_f', 'gan_g']
    
    #   D_x: discriminator to discriminate real x from fake x
    #   D_y: discriminator to discriminate real y from fake y
    #   G_x: generator to generate fake x
    #   G_y: generator to generate fake y
    self.model_names = ['D_x', 'D_y', 'G_x', 'G_y']
    self.netD_x = define_D()
    self.netD_y = define_D()
    self.netG_x = define_G()
    self.netG_y = define_G()

    self.criterionGAN = GANLoss()
    self.criterionCycle = torch.nn.L1Loss()
    
    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizers.append(self.optimizer_G)
    self.optimizers.append(self.optimizer_D)

  def set_requires_grad(self, nets, requires_grad=False):
        """Set requies_grad=Fasle for all the networks to avoid unnecessary computations
        Parameters:
            nets (network list)   -- a list of networks
            requires_grad (bool)  -- whether the networks require gradients or not
        """
        if not isinstance(nets, list):
            nets = [nets]
        for net in nets:
            if net is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

  def forward(self, x):
    self.fake_x = self.netG_x(self.real_y)
    self.rec_y = self.netG_y(self.fake_x)   # rec_y = G_y(G_x(y))
    self.fake_y = self.netG_y(self.real_x)
    elf.rec_x = self.netG_x(self.fake_y)    # rec_x = G_x(G_y(x))
  
  def backward_D_basic(self, netD, real, fake):
    # real
    pred_real = netD(real)
    loss_D_real = self.criterionGAN(pred_real)

    # fake
    pred_fake = netD(fake)
    loss_D_fake = self.criterionGAN(pred_fake)

    # combined loss and calculate gradients
    loss_D = (loss_D_real + loss_D_fake) / 2
    loss_D.backward()
    return loss_D
  
  def backward_D_x(self):
    fake_x = self.fake_A_pool.query(self.fake_x)
    self.loss_D_x = self.backward_D_basic(self.netD_x, self.real_x, fake_x)

  def backward_D_y(self):
    fake_y = self.fake_A_pool.query(self.fake_y)
    self.loss_D_y = self.backward_D_basic(self.netD_y, self.real_y, fake_y)

  def backward_G(self):
    """Calculate the loss for generators G_A and G_B"""
    lambda_x = self.opt.lambda_x
    lambda_y = self.opt.lambda_y

    # GAN loss D_x(G_x(y))
    self.loss_G_x = self.criterionGAN(self.netD_x(self.fake_x))
    # GAN loss D_y(G_y(x))
    self.loss_G_y = self.criterionGAN(self.netD_y(self.fake_y))
    # Forward cycle loss || G_x(G_y(x)) - x||
    self.loss_cycle_x = self.criterionCycle(self.rec_x, self.real_x) * lambda_x
    # Backward cycle loss || G_y(G_x(y)) - y||
    self.loss_cycle_y = self.criterionCycle(self.rec_y, self.real_y) * lambda_y
    # combined loss and calculate gradients
    self.loss_G = self.loss_G_x + self.loss_G_y + self.loss_cycle_x + self.loss_cycle_y
    self.loss_G.backward()

  def optimize_parameters(self):

        """Calculate losses, gradients, and update network weights; called in every training iteration"""
        # forward
        self.forward()      # compute fake images and reconstruction images.
        # G_x and G_y
        self.set_requires_grad([self.netD_x, self.netD_y], False)  # Ds require no gradients when optimizing Gs
        self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero
        self.backward_G()             # calculate gradients for G_A and G_B
        self.optimizer_G.step()       # update G_A and G_B's weights
        # D_x and D_y
        self.set_requires_grad([self.netD_x, self.netD_y], True)
        self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero
        self.backward_D_x()      # calculate gradients for D_A
        self.backward_D_y()      # calculate graidents for D_B
        self.optimizer_D.step()  # update D_A and D_B's weights