# -*- coding: utf-8 -*-
"""CycleGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rzaEBv6J7s--a4NCtxFjItAQJOMDGHtE

# Import Libraries
"""

import torchvision
import torch
import itertools
import numpy as np
from torch.utils.data.sampler import SubsetRandomSampler
import time
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
from torch.optim import lr_scheduler
import matplotlib.pyplot as plt
import random
import torch

"""# CycleGAN Utils"""

class ImagePool():

    def __init__(self, pool_size):
        self.pool_size = pool_size
        if self.pool_size > 0: 
            self.num_imgs = 0
            self.imgs = []

    def query(self, imgs):
        if self.pool_size == 0:
            return imgs
        return_imgs = []
        for image in imgs:
            image = torch.unsqueeze(image.data, 0)
            if self.num_imgs < self.pool_size:
                self.num_imgs = self.num_imgs + 1
                self.imgs.append(image)
                return_imgs.append(image)
            else:
                p = random.uniform(0, 1)
                if p > 0.5: 
                    random_id = random.randint(0, self.pool_size - 1) 
                    tmp = self.imgs[random_id].clone()
                    self.imgs[random_id] = image
                    return_imgs.append(tmp)
                else:   
                    return_imgs.append(image)
        return_imgs = torch.cat(return_imgs, 0)  
        return return_imgs

class GANLoss(nn.Module):
    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):
        super(GANLoss, self).__init__()
        self.register_buffer('real_label', torch.tensor(target_real_label))
        self.register_buffer('fake_label', torch.tensor(target_fake_label))
        self.gan_mode = gan_mode
        # print("fake x pool set to : " + str(self.fake_x_pool))
        if gan_mode == 'lsgan':
            self.loss = nn.MSELoss()
        elif gan_mode == 'vanilla':
            self.loss = nn.BCEWithLogitsLoss()
        elif gan_mode in ['wgangp']:
            self.loss = None
        else:
            raise NotImplementedError('gan mode %s not implemented' % gan_mode)

    def get_target_tensor(self, prediction, target_is_real):
        if target_is_real:
            target_tensor = self.real_label
        else:
            target_tensor = self.fake_label
        return target_tensor.expand_as(prediction)

    def __call__(self, prediction, target_is_real):
        if self.gan_mode in ['lsgan', 'vanilla']:
            target_tensor = self.get_target_tensor(prediction, target_is_real)
            loss = self.loss(prediction, target_tensor)
        return loss

"""# CycleGAN Architecture"""

class CycleGAN(nn.Module):
  def __init__(self, name, opt):
    super(CycleGAN, self).__init__()
    self.name = "CycleGAN" + name

    self.opt = opt
    self.gpu_ids = opt.gpu_ids
    self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')

    #   cycle_x: loss of generating fake x
    #   cycle_y: loss of generating fake x
    #   gan_f: loss of discriminating real x
    #   gan_g: loss of discriminating real y
    self.loss_names = ['cycle_x', 'cycle_y', 'gan_f', 'gan_g']

    self.fake_x_pool = ImagePool(opt.pool_size)
    self.fake_y_pool = ImagePool(opt.pool_size)
    
    #   D_x: discriminator to discriminate real x from fake x
    #   D_y: discriminator to discriminate real y from fake y
    #   G_x: generator to generate fake x
    #   G_y: generator to generate fake y
    self.model_names = ['D_x', 'D_y', 'G_x', 'G_y']
    self.netD_x = MonaiDiscriminator(3)
    self.netD_y = MonaiDiscriminator(3)
    self.netG_x = MonaiGenerator(3, 3, 64, 6, 'instance', True)
    self.netG_y = MonaiGenerator(3, 3, 64, 6, 'instance', True)

    #   criterions
    self.criterionGAN = GANLoss(opt.gan_mode).to(self.device)
    self.criterionCycle = torch.nn.L1Loss()
    
    #   optimizers
    self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_x.parameters(), self.netG_y.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_x.parameters(), self.netD_y.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))
    self.optimizers = []
    self.optimizers.append(self.optimizer_G)
    self.optimizers.append(self.optimizer_D)

    #   learning rate scheduler
    self.schedulers = [self.get_scheduler(optimizer, opt) for optimizer in self.optimizers]

    #   training records
    self.save_progress = []
    self.all_loss_D_fake = []
    self.all_loss_D_real = []
    self.all_loss_D = []
    self.all_loss_G = []
    self.all_loss_G_x = []
    self.all_loss_G_y = []
    self.all_loss_cycle_x = []
    self.all_loss_cycle_y = []

  def get_scheduler(self, optimizer, opt):
    if opt.lr_policy == 'linear':
        def lambda_rule(epoch):
            lr_l = 1.0 - max(0, epoch - opt.n_epochs)
            return lr_l
        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)
    elif opt.lr_policy == 'step':
        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)
    return scheduler
  
  def update_learning_rate(self):
        old_lr = self.optimizers[0].param_groups[0]['lr']
        for scheduler in self.schedulers:
            if self.opt.lr_policy == 'plateau':
                scheduler.step(self.metric)
            else:
                scheduler.step()

        new_lr = self.optimizers[0].param_groups[0]['lr']
        print('learning rate %.7f -> %.7f' % (old_lr, new_lr))

  def set_input(self, input):
        XtoY = self.opt.direction == 'XtoY'
        self.real_x = input['X' if XtoY else 'Y']
        self.real_y = input['Y' if XtoY else 'X']

  def set_requires_grad(self, networks, requires_grad=False):
        if not isinstance(networks, list):
            networks = [networks]
        for network in networks:
            if network is not None:
                for param in net.parameters():
                    param.requires_grad = requires_grad

  def forward(self):
    self.fake_x = self.netG_x(self.real_y)
    self.rec_y = self.netG_y(self.fake_x)   # rec_y = G_y(G_x(y))
    self.fake_y = self.netG_y(self.real_x)
    self.rec_x = self.netG_x(self.fake_y)    # rec_x = G_x(G_y(x))
  
  def backward_D_basic(self, netD, real, fake):
    # real
    pred_real = netD(real)
    self.loss_D_real = self.criterionGAN(pred_real, True)

    # fake
    pred_fake = netD(fake)
    self.loss_D_fake = self.criterionGAN(pred_fake, False)
    
    # combined loss and calculate gradients
    self.loss_D = (self.loss_D_real + self.loss_D_fake) / 2
    self.loss_D.backward()
    return self.loss_D
  
  def backward_D_x(self):
    # Calculate the loss for generators D_x
    fake_x = self.fake_x_pool.query(self.fake_x)
    self.loss_D_x = self.backward_D_basic(self.netD_x, self.real_x, fake_x)

  def backward_D_y(self):
    # Calculate the loss for generators D_y
    fake_y = self.fake_y_pool.query(self.fake_y)
    self.loss_D_y = self.backward_D_basic(self.netD_y, self.real_y, fake_y)

  def backward_G(self):
   # Calculate the loss for generators G_A and G_B
    lambda_x = self.opt.lambda_x
    lambda_y = self.opt.lambda_y

    # GAN loss D_x(G_x(y))
    self.loss_G_x = self.criterionGAN(self.netD_x(self.fake_x), False)
    # GAN loss D_y(G_y(x))
    self.loss_G_y = self.criterionGAN(self.netD_y(self.fake_y), False)
    self.loss_cycle_x = self.criterionCycle(self.rec_x, self.real_x) * lambda_x
    self.loss_cycle_y = self.criterionCycle(self.rec_y, self.real_y) * lambda_y
    
    # combined loss and calculate gradients
    self.loss_G = self.loss_G_x + self.loss_G_y + self.loss_cycle_x + self.loss_cycle_y
    self.loss_G.backward()
  
  def record_loss(self):
    self.all_loss_D_fake.append(self.loss_D_fake)
    self.all_loss_D_real.append(self.loss_D_real)
    self.all_loss_D.append(self.loss_D)
    self.all_loss_G.append(self.loss_G)
    self.all_loss_G_x.append(self.loss_G_x)
    self.all_loss_G_y.append(self.loss_G_y)
    self.all_loss_cycle_x.append(self.loss_cycle_x)
    self.all_loss_cycle_y.append(self.loss_cycle_y)
  
  def record_output_imgs(self):
    # save progress
    progress = {}
    progress["real_x"] = self.real_x
    progress["real_y"] = self.real_y
    progress["fake_x"] = self.fake_x
    progress["rec_y"] = self.rec_y
    progress["fake_y"] = self.fake_y
    progress["rec_x"] = self.rec_x
    self.save_progress.append(progress)

  def optimize_parameters(self):
    # forward
    self.forward() 
    self.record_output_imgs() 

    # G_x and G_y
    self.set_requires_grad([self.netD_x, self.netD_y], False)  
    self.optimizer_G.zero_grad() 
    self.backward_G()  
    self.optimizer_G.step()  
    # D_x and D_y
    self.set_requires_grad([self.netD_x, self.netD_y], True)
    self.optimizer_D.zero_grad()  
    self.backward_D_x()    
    self.backward_D_y()   

    self.record_loss()   
    self.optimizer_D.step()  